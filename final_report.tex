% Copied this sample latex file from https://www.usenix.org/conferences/author-resources/paper-templates
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,endnotes}
\begin{document}

%don't want date printed
\date{}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf Kaggle Challenge: Predicting Rossmann Store Sales}

\author{
  {\rm Mingu Jo}\\
  University of California, Berkeley
  \and
      {\rm Wonjohn Choi}\\
      University of California, Berkeley
} % end author

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}


\section{Abstract}
Our project attempted to apply linear regression and various machine learning techniques to a real-world problem of predicting drug store sales. Rossmann, Germany's second-largest drug store chain, has provided past sales information of 1115 Rossmann stores located across Germany. We preprocessed, feature-engineered the data, and examined 4 different statistical / machine learning analysis for forecasting sales of each store: multivariate linear regression, random forest, gradient boosting, and TODO. Then, we compared the methods' predictive power by computing Root Mean Square Percentage Error (RMSPE). We found that TODO:X performed the best with a RMSPE score of TODO:XXX.

\section{Introduction}
The objective of this project is to predict 6 weeks of daily sales for 1115 Rossmann stores located across Germany using the data which was provided by Rossmann through Kaggle. The motivation of this project is the following: by reliably predicting daily sales, store managers may decrease operational costs, and create effective staff schedules (ex. by assigning more staffs during busy days). Also, we would like to identify which type of techniques be effective in a real-world sales prediction task. Before building any regression or machine learning models for the prediction, we attempted to define which features are significant to determine daily sales of stores. By performing exploratory data analysis, we found how some features such as promotion and competitor distance affect the sales. % sounds little strange

(TODO: literature review) Is the literature review in the Introduction informative and organized?
To confirm our assumption and apply various techniques for the prediction, we first explored several similar efforts that made for sales prediction task in the past in order to set the basis of our study. 

- Linear Regression: The professor at Western Illionois University suggests multiple linear regression with feature selection and multicollinearity diagnostics to be a popuplar technique in sales forecasting. This model was applied for predicting the number of subscribers that a cable station can expect, which is closely related to our task. We decided to apply this technique along with partial t-test and stepwise regression to select significant features. 

- Random Forest: the random forest is a tree-based ensemble method where all trees depend on a collection of random variables. (Breiman). Breiman suggests that random forest algorithm is actually from a classification viewpoint but the approach is
applicable to regression as well. Therefore, we consider the algorithm is also applicable to our regression task. Nils Adriansson and Ingrid Mattsson in their thesis applies the algorithm into a GDP forecast task, showing its power in time-series forecasting (Adriansson). We assume this algorithm can outperform linear regression because Breiman states that the RF does not need to assume any distribution for variables. (Breiman)

- Gradient Boosting: We also found useful article by Jay Gu on gradient boosting regression. In his article, he asserted that, for predictive analytics, the gradient boosted trees model (GBM) is one of the most effective machine learning models. As he did, we have also witnessed a good number of winning solutions for various kaggle competitions that contain boosted tree models as a crucial component. We got the basic framework from the textbook (    ). Especially, we noticed that Extreme Gradient Boosting (xgboost) is similar to gradient boosting framework but more popular and efficient. We decided to fine-tune our dataset and apply the algorithm on it for better prediction.


\subsection{Description of Datasets}
Data sets are given by Rossmann through Kaggle. There are three files: test.csv, train.csv, and store.csv. train.csv contains 1017209 observed daily sales of different stores from 2013 to 2015. store.csv contains supplementary information for each store (41088 lines because there are 41088 stores). train.csv and store.csv both contain Store id that we can use to join the train and store data sets. Finally, test.csv has 41088 lines of daily sales of different stores but the value of Sales column is omitted. We are expected to predict the value of Sales column for the test set (test.csv) by using the training set (train.csv) and store data (store.csv).

The following describes fields in test.csv and training.csv:
\begin{itemize}
\item Store - the unique id for each store (positive integer ranging from 1 to 1115)
\item DayOfWeek - the day of week (positive integer ranging from 1 to 7 where 1, 2, ..., 7 correspond to Monday, Tuesday, ..., Sunday)
\item Date - the date (string of format YYYY­MM­DD)
\item Sales - the total amount of revenue for this given day (positive number between 0 and 41551). This value is what needs to be predicted, so only exists in training.csv.
\item Customers - the number of customers on the given day. This also only exists in training.csv.
\item Open - an indicator of whether the store was open on the given day (0 if closed, 1 if open)
\item Promo - an indicator of whether the store was running a promotion on the given day (0 if not running, 1 if running)
\item StateHoliday - an indicator of whether the given day was a state holiday (0 if not a state holiday, a if public holiday, b if eastern holiday, c if christmas)
\item SchoolHoliday - an indicator of whether the store was affected by a nearby school holiday on the given day (0 if not affected, 1 if affected)
\end{itemize}

The following describes fields in store.csv:
\begin{itemize}
\item Store - the unique id for each store (positive integer ranging from 1 to 1115)
\item StoreType - the store's model (4 types: a, b, c, d)
\item CompetitionDistance - the distance in meters to the nearest competing store (positive number franging from 20.0 to 75860.0, and NA's if no competitors)
\item Assortment - the store's assortment level (a = basic, b = extra, c = extended)
\item CompetitionOpenSinceMonth - the approximate month of the time when the nearest competing store opened (positive integer ranging from 1 to 12, and NA's if no competitors)
\item CompetitionOpenSinceYear - the approximate year of the time when the nearest competing store opened (positive integer ranging from 1900 to 2015, and NA's if no competitors)
\item Promo2 - an indicator of whether the store was participating in a promotion that runs over a period of time  (0 if not participating, 1 if participating)
\item Promo2SinceWeek - the approximate week of year when the store started to participate in promo2 (positive integer ranging from 1 to 50, and NA's if no promo2)
\item Promo2SinceYear - the approximate year of time when the store started to participate in promo2 (positive integer ranging from 2009 to 2015, and NA's if no promo2)
\item PromoInterval - the months when the promo2 started (comma-separated list of string months. ex. ``Feb,May,Aug,Nov'')
\end{itemize}

\subsection{Preprocessing}

For preprocessing, we have done the followings:
\begin{itemize}
\item We merged train.csv and store.csv by ``Store'' because we can predict daily sales better with more data related to the sales. We also merged test.csv and store.csv by ``Store''.
\item We removed ``Customers'' field from the training data because it does not exist in the test data so we cannot use ``Customers'' to predict ``Sales''.
\item For ``CompetitionDistance'' with NA values, we replaced NA with 1000000. Since 1000000 is much more bigger than any other distances, replacing NA with the number simulated an effect of having no competitor. In the similar way, we filled missing values for ``CompetitionOpenSinceYear'' and ``CompetitionOpenSinceMonth''.
\item Similarly, for ``Promo2SinceYear'' with NA valus, we replaced NA with 1990 to simulate the effect of having no promo2.
\item We splitted ``Date'' field into three fields: ``year'', ``month'', and ``day''.
\item We computed average sales for each store to create a new field ``Average.Sales''.
\item We noticed the feature ``CompetitionDistance'' is highly skewed to right. For the sake of linear regression, we took log transformation on the feature ``CompetitionDistance'' to create a new field ``LogCompetitionDistance'' to make sure that the relationship between explanatory variables and dependent variable is approximately linear.
\end{itemize}

% TODO:add the CompetitionDistance graph here. 

\subsection{Exploratory Data Analysis}
To check any distinctive relationships between variables, we drew a pearson correlation coefficient plot using corrplot library. There are several interesting facts we captured. 
- \"DayOfWeek\" and \"Sales\" are negatively correlated. This implies that, as \"DayOfWeek\" approaches to Saturday and Sunday, sales would decrease because most drugstores would close on these days.
- \"LogCompetitionDistance\" and \"Sales\" are negatively correlated. In other words, lower distance to the competitor implies slightly higher sales. We assume this could occur because stores with a close distance to the competitor are mostly located in crowded regions such as a downtown city with higher sales overall.

% TODO:add the correlation plot here. 



\section{Methods}
% #####???????????
We started off by setting a benchmark to obtain a baseline for the prediction. Then, we took 3 different approaches to outperform the benchmark and former model.  

\subsection{5-fold Cross Validation} %j???????????
To generalize our predictions to limit problems of overfitting on the training set, we decided to ramdomly partition the original training observations into 5 equal sized subsamples. The training set is comprised of 4 equal sized subsamples and the rest as the validation set. For the purpose of reproducibility and comparison of the predictive powers of different models, we set the seed (with a value of 42). Our general process would be as the following:
1. Repeat cross-validation process 5 times with each of the 5 subsamples used exactly once as the validation set.
2. Compute the average of 5 different validation error rate. 
3. Predict sales on the test set using the model. 
4. Submit on Kaggle website to get the test error rate.
5. Compare the average validation error rate and the test error rate among differnent models.


\subsection{Error Metric}
As the metrics that Kaggle uses to compute test error is the Root Mean Squared Percentage Error (RMSPE), we used the same metrics to compute our validation error. It's the following:

$RMSPE = \sqrt{\frac{1}{n} \sum {(\frac{y_i - \hat y_i}{y_i})}^2}$

where ${y_i}$ is a store's sales on a single day, ${yhat_i}$ is the daily sales prediction for the store on the day, and ${n}$ is the number of (store, date) pairs in the data set. 

\subsection{Benchmark}
Our benchmark is simply the average sales of each store. Precisely, we predict daily sales of a store for a day by the average daily sales the store had in the training data set. This method does not account for many factors (whether the day was holiday, whether the store was running a promotion on the day, etc), so we expected its error rate to have a higher error rate than any other methods we applied. This method had a the validation error rate of TODO and the test error rate of TODO. 

\subsection{Linear Regression}
To apply linear Regression on the data, we decided to use R's $lm$ function. $lm$ is an implemention of multivariate linear regression. If we supply $Y \sim X$ as the input, $lm$ builds and returns a model that can be used to predict $Y$ using $X$. Under the hood, it basically solves the normal equation $\beta = (X^T X)^{-1} X^T Y$.

We first attempted to use all features available. With all features (Holiday, ..., promo2), we obtained a validation error rate of TOD and the test error rate of TODO.

\subsubsection{Feature Selection}
Before working on feature select, we took a look at the resulting lm with R function $summary(lm)$ to get some general idea of how important each variable is. DayOfWeek, Open, Promo, StateHoliday, SchoolHoliday, StoreType, Assortment, Promo2SinceWeek, Average.Sales, LogCompetitionDistance, day, month, and year seemed to have high t-value with p-value less than 0.05. This intuitively made sense because each variable had some ways to affect daily sales:
\begin{enumerate}
\item DayOfWeek: On Sunday, 
\item StateHoliday: On a holiday, it makes sense that more people come to shop. Indeed, the coefficient is TODO.
\item sdfas
\end{enumerate}

To remove useless features, we first applied Akaike information criterion (AIC) by applying R function $stepAIC$. However, this failed to remove any features, so decided to manually remove features with p value more than 0.05. This resulted in a linear model with a validation error rate of TODO and test error rate of TODO.

\subsubsection{Lasso}

\subsubsection{Ridge}


\subsection{Random Forest}
Random Forest is an ensemble learning method that constructs decisions tree using training set. By aggregating many decision trees, Random Forest can make predictions while avoiding overfitting. Here is our Random Forest Regression steps: \\
1. Construct random forest trees (number of trees is defined by hyper parameter)
2. Classify the incoming data into the decision tree node
3. Calculate the mean value for each node for prediction
In particular, we used H2O’s RandomForest package to carry out the training and prediction. Then, we optimized parameters to improve on our prediction model. H2O is the open source math engine for huge dataset (TODO). Since H2O package enables us to compute Random Forest Regression algorithm in parallel distribution, we decided to use the package for faster computation compared to R's Random Forest package.

TODO HYPER PARAMETERS


\subsection{Gradient Boosting}
The similarity between Random Forest and Gradient Boosting is that both generate decision trees. However, Random forest uses bootstrapping method for training while Gradient Boosting builds decision tree over the residuals. In other words, Gradient Boosting Tree generates the decision tree as it optimizes on a proper loss function (   ). This refers that we can establish our own loss function for optimization, while we couldn't do in Random Forest. We set our loss function to be our error metric, RMPSE. Our decision trees here are regression trees. Each regression tree divides the data point at each non-leaf node according to a constraint on a feature. The leaf node value of a data point decides its score, and a data point is predicted by averaging the scores of the leaf node it belongs to.

In particular, we used R's xgboost package. XGBoost also known as Extreme Gradient Boosting is a powerful supervised learning algorithm (). This package enables an automatic parallel computation on a single machine. For our regression task, we put \"reg:linear\" as a parameter. Our training objective here is : 식 




\section{Results}

\subsection{Random Forest}
The two main  parameters  we  tuned for RF  is  the number  of  trees and the size  of  the random  subsets of  features  to  
consider  when  splitting a node. We  used  5 fold cross validation  to  get RMSPE while varying these parameters. Two plots 
are shown below.  From  these plots,  we  could see that  RMSPE doesn’t change  too much  after tree  number  reaches 30  and 
after feature number  reaches 20.




\subsection{Gradient Boosting}
Before comparing the results of gradient boosting with other algorithms we performed, let's explore on the results of gradient boosting itself only. As we already feature engineered our data set, we did not perform additional feature engineering on the training data for this algorithm. As we did in previous methodologies, we tried a range of specific parameters to maximize its prediction accuracy. Since the major issue of all machine learning tasks is to prevent overfitting on training data, we also use 5-fold cross validation here. We computed the average error rate as a parameter changes, and graphed them to choose the best parameter. There are actually more parameters to tune in Gradient Boosting. There are three major parameters subject to be tuned: the learning rate, maximum depth, and number of trees. In here, we decided to use the default number of trees, and tune the learning rate and the maximum depth. The effects of tuning are visualized in the following graphs:


As the error rate shows, Gradient Boosting model gives us the most predictive power by far. We were able to achieve test error rate (RMPSE) of 0.123.

- learning rate


- maximum depth


- Time


Overall, we could check that compared to random forest, it is more likely to overfit. 


you potentially increase the size of the tree and get better precision, at the risk of overfitting and longer training time.




Unfortunately, Gradient Boosting Tree has its own drawbacks.
First, It is more likely to overfit than random forest.
It trusts every data point and tries to find optimal linear combination
of trees given the train data. Second, there are more
parameters in Gradient Boosting Tree. We have to tune the
number of trees, maximum depth and the learning rate at the
same time. For the first drawback, we can try to avoid over-
fitting by using the cross validation provided by sklearn. For
the second method, we have to spend more time to tune it,
which is tradeoff between time and accuracy

One of the major problems of all machine learning algorithms is to know when to stop, i.e., how to prevent the learning algorithm to fit esoteric aspects of the training data that are not likely to improve the predictive validity of the respective model. This issue is also known as the problem of overfitting.

The gradient boosted trees model is a type of additive model that makes predictions by combining decisions from a sequence of base models. More formally we can write this class of models as:

g(x)=f0(x)+f1(x)+f2(x)+...
g(x)=f0(x)+f1(x)+f2(x)+...

Random forest runtimes are quite fast, and they are able to deal with unbalanced and missing data. Random Forest weaknesses are that when used for regression they cannot predict beyond the range in the training data, and that they may over-fit data sets that are particularly noisy.



\section{Discussion}
To visualize the overall performance of our different models, we organized the resulting error rates into a simple table. 

% TODO: make an overall error rate table

- F


- Performance
As shown in the table, our highest performing model was the Gradient Boosting model. This model provided us the lowest testing error rate of \"    \". The worst performing model was the linear regression model. 

As we expected, linear regression model did not perfrom very well because of the non-linearity of the data. 

However, we checked the sharp increase from linear regression to Random Forest model. we confirmed that tree based models can approximate functions with any \"non-linear shape\", whereas linear models can produce functions with a \"linear shape\" with respect to a chosen set of features. 


- Time (complexity)
As shown in the table, it took more than 5 hours to train Gradient Boosting model and compute different error rates according to different parameters over 5-fold cross validation.



Random Forest is able to discover more complex dependencies at the cost of more time for fitting




We were able to achieve 12.3% rms error using
gradient boosting, our main algorithm, to predict
Rossmann sales without using customer data.
Boosting worked very well for this data set, most
likely due to the fact that this data is extremely
dense, which is known to be ideal for boosting.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Now we're going to cite somebody.  Watch for the cite tag.
Here it comes~\cite{Chaum1981,Diffie1976}.  The tilde character (\~{})
in the source means a non-breaking space.  This way, your reference will
always be attached to the word that preceded it, instead of going to the
next line.

\section{Supplementary Methods}
\subsection{First SubSection}

Here's a typical figure reference.  The figure is centered at the
top of the column.  It's scaled.  It's explicitly placed.  You'll
have to tweak the numbers to get what you want.\\

% you can also use the wonderful epsfig package...
\begin{figure}[t]
\begin{center}
\begin{picture}(300,150)(0,200)
\put(-15,-30){\special{psfile = fig1.ps hscale = 50 vscale = 50}}
\end{picture}\\
\end{center}
\caption{Wonderful Flowchart}
\end{figure}

This text came after the figure, so we'll casually refer to Figure 1
as we go on our merry way.

\subsection{New Subsection}

It can get tricky typesetting Tcl and C code in LaTeX because they share
a lot of mystical feelings about certain magic characters.  You
will have to do a lot of escaping to typeset curly braces and percent
signs, for example, like this:
``The {\tt \%module} directive
sets the name of the initialization function.  This is optional, but is
recommended if building a Tcl 7.5 module.
Everything inside the {\tt \%\{, \%\}}
block is copied directly into the output. allowing the inclusion of
header files and additional C code." \\

Sometimes you want to really call attention to a piece of text.  You
can center it in the column like this:
\begin{center}
{\tt \_1008e614\_Vector\_p}
\end{center}
and people will really notice it.\\

\noindent
The noindent at the start of this paragraph makes it clear that it's
a continuation of the preceding text, not a new para in its own right.


Now this is an ingenious way to get a forced space.
{\tt Real~$*$} and {\tt double~$*$} are equivalent. 

Now here is another way to call attention to a line of code, but instead
of centering it, we noindent and bold it.\\

\noindent
{\bf \tt size\_t : fread ptr size nobj stream } \\

And here we have made an indented para like a definition tag (dt)
in HTML.  You don't need a surrounding list macro pair.
\begin{itemize}
\item[]  {\tt fread} reads from {\tt stream} into the array {\tt ptr} at
most {\tt nobj} objects of size {\tt size}.   {\tt fread} returns
the number of objects read. 
\end{itemize}
This concludes the definitions tag.

\subsection{How to Build Your Paper}

You have to run {\tt latex} once to prepare your references for
munging.  Then run {\tt bibtex} to build your bibliography metadata.
Then run {\tt latex} twice to ensure all references have been resolved.
If your source file is called {\tt usenixTemplate.tex} and your {\tt
  bibtex} file is called {\tt usenixTemplate.bib}, here's what you do:
{\tt \small
\begin{verbatim}
latex usenixTemplate
bibtex usenixTemplate
latex usenixTemplate
latex usenixTemplate
\end{verbatim}
}


\subsection{Last SubSection}

Well, it's getting boring isn't it.  This is the last subsection
before we wrap it up.


\section{Results}

\section{Supplementary Results}

A polite author always includes acknowledgments.  Thank everyone,
especially those who funded the work. 

\section{Discussion}

It's great when this section says that MyWonderfulApp is free software, 
available via anonymous FTP from

\begin{center}
{\tt ftp.site.dom/pub/myname/Wonderful}\\
\end{center}

Also, it's even greater when you can write that information is also 
available on the Wonderful homepage at 

\begin{center}
{\tt http://www.site.dom/\~{}myname/SWIG}
\end{center}

Now we get serious and fill in those references.  Remember you will
have to run latex twice on the document in order to resolve those
cite tags you met earlier.  This is where they get resolved.
We've preserved some real ones in addition to the template-speak.
After the bibliography you are DONE.

\section{References Cited}

Dehkordi-Vakil, Farideh. "Multiple Regression Analysis." DS-533. Illionis, Macomb. Lecture.

Breiman, L., (2001a). ”Random Forests”, Machine Learning 45 (1) pp. 5-32.
R.D. https://www.diva-portal.org/smash/get/diva2:785776/FULLTEXT01.pdf


https://documents.software.dell.com/statistics/textbook/boosting-trees-regression-classification

h2o package description


\section{Author Contributions}

\end{document}











Some embedded literal typset code might 
look like the following :

{\tt \small
\begin{verbatim}
int wrap_fact(ClientData clientData,
              Tcl_Interp *interp,
              int argc, char *argv[]) {
    int result;
    int arg0;
    if (argc != 2) {
        interp->result = "wrong # args";
        return TCL_ERROR;
    }
    arg0 = atoi(argv[1]);
    result = fact(arg0);
    sprintf(interp->result,"%d",result);
    return TCL_OK;
}
\end{verbatim}
}



