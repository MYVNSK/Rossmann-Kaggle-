% Copied this sample latex file from https://www.usenix.org/conferences/author-resources/paper-templates
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,endnotes}
\begin{document}

%don't want date printed
\date{}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf Kaggle Challenge: Predicting Rossmann Store Sales}

\author{
  {\rm Mingu Jo}\\
  University of California, Berkeley
  \and
      {\rm Wonjohn Choi}\\
      University of California, Berkeley
} % end author

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}


\section{Abstract}
The objective of this project is to predict 6 weeks of daily sales for 1115 Rossmann stores located across Germany using data about stores' past daily sales, competitor, etc (the list of available data will be introduced more in depth later in this paper). The data was provided by Rossmann through Kaggle. The motivation of this project is the following: by reliably predicting daily sales, store managers can create effective staff schedules (by assigning more staffs during busy days). In order to predict daily sales, we first preprocessed the data and applied several methods (multivariate linear regression, random forest, generalized linear regression, etc). Then, we compared the methods' predictive power by computing Root Mean Square Percentage Error (RMSPE). We found that TODO:X performed the best with a RMSPE score of TODO:XXX.

\section{Introduction}

(TODO: literature review) Is the literature review in the Introduction informative and organized?
text. \\

More fascinating text. Features\endnote{Remember to use endnotes, not footnotes!} galore, plethora of promises.\\

\subsection{Dataset and Preprocessing}
Training and test dataset are given by Rossmann through Kaggle. The training set is composed of two different parts: one is \~10 million observed daily sales of different stores from 2013 to 2015, and the other is supplementary information for each store. We merged the two different parts by stores to have the training set. The training set had originally 15 features, but we extended to 19. We separated out year, month of the year, and day of the month from the feature \"date\". 

% TODO:add the feature explanatory table here.

Also, we computed average sales for each store. In addition, we noticed the feature \"CompetitionDistance\" is highly skewed to right. For the sake of linear regression, we decided to take log transformation on the feature \"CompetitionDistance\" to make sure that the relationship between explanatory variables and dependent variable is approximately linear.

% TODO:add the CompetitionDistance graph here. 

Because of several missing values in the training set, we filled them based on our logic.
- There are some stores on some date which do not have any competitors. In this case, the competitor information such as \"CompetitionDistance\" for this specific observation are missing. Therefore, we assigned some big number as 1000000 for \"CompetitionDistance\" to weaken its impact. In the similar way, we filled missing values for \"CompetitionOpenSinceYear\" and CompetitionOpenSinceMonth
- There are some stores on some date which are not in the continuing promotion. In this case, the feature \"Promo2\" is 0, and Promo2Since\[Year/Week\] are missing. We filled them with the lowest possible value to weaken their impact. 


\subsection{Exploratory Data Analysis}
To check any distinctive relationships between variables, we drew a pearson correlation coefficient plot using corrplot library. There are several interesting facts we captured. 
- \"DayOfWeek\" and \"Sales\" are negatively correlated. This implies that, as \"DayOfWeek\" approaches to Saturday and Sunday, sales would decrease because most drugstores would close on these days.
- \"LogCompetitionDistance\" and \"Sales\" are negatively correlated. In other words, lower distance to the competitor implies slightly higher sales. We assume this could occur because stores with a close distance to the competitor are mostly located in crowded regions such as a downtown city with higher sales overall.

% TODO:add the correlation plot here. 



\section{Methods}
% #####???????????
We started off by setting a benchmark to obtain a baseline for the prediction. Then, we took 3 different approaches to outperform the benchmark and former model.  

\subsection{5-fold Cross Validation} %j???????????
To generalize our predictions to limit problems of overfitting on the training set, we decided to ramdomly partition the original training observations into 5 equal sized subsamples. The training set is comprised of 4 equal sized subsamples and the rest as the validation set. For the purpose of reproducibility and comparison of the predictive powers of different models, we set the seed. Our general process would be as the following:
1. Repeat cross-validation process 5 times with each of the 5 subsamples used exactly once as the validation set.
2. Compute the average of 5 different validation error rate. 
3. Predict sales on the test set using the model. 
4. Submit on Kaggle website to get the test error rate.
5. Compare the average validation error rate and the test error rate among differnent models.


\subsection{Error Metric}
As the metrics that Kaggle uses to compute test error is the root-mean-squared-percentage-error (RMSPE), we used the same metrics to compute our validation error. It's the following:

% TODO:add the RMSPE equation here.

where ${y_i}$ is a store's sales on a single day, ${yhat_i}$ is the prediction, and ${n}$ is the number of days. 

\subsection{Benchmark}
Our benchmark is simply the average sales of each store. This gives us the validation error rate of : and the test error rate of : 


\subsection{Linear Regression}


\subsubsection{Feature Selection}

\subsubsection{Lasso}



\subsection{Random Forest}





Some embedded literal typset code might 
look like the following :

{\tt \small
\begin{verbatim}
int wrap_fact(ClientData clientData,
              Tcl_Interp *interp,
              int argc, char *argv[]) {
    int result;
    int arg0;
    if (argc != 2) {
        interp->result = "wrong # args";
        return TCL_ERROR;
    }
    arg0 = atoi(argv[1]);
    result = fact(arg0);
    sprintf(interp->result,"%d",result);
    return TCL_OK;
}
\end{verbatim}
}

Now we're going to cite somebody.  Watch for the cite tag.
Here it comes~\cite{Chaum1981,Diffie1976}.  The tilde character (\~{})
in the source means a non-breaking space.  This way, your reference will
always be attached to the word that preceded it, instead of going to the
next line.

\section{Supplementary Methods}
\subsection{First SubSection}

Here's a typical figure reference.  The figure is centered at the
top of the column.  It's scaled.  It's explicitly placed.  You'll
have to tweak the numbers to get what you want.\\

% you can also use the wonderful epsfig package...
\begin{figure}[t]
\begin{center}
\begin{picture}(300,150)(0,200)
\put(-15,-30){\special{psfile = fig1.ps hscale = 50 vscale = 50}}
\end{picture}\\
\end{center}
\caption{Wonderful Flowchart}
\end{figure}

This text came after the figure, so we'll casually refer to Figure 1
as we go on our merry way.

\subsection{New Subsection}

It can get tricky typesetting Tcl and C code in LaTeX because they share
a lot of mystical feelings about certain magic characters.  You
will have to do a lot of escaping to typeset curly braces and percent
signs, for example, like this:
``The {\tt \%module} directive
sets the name of the initialization function.  This is optional, but is
recommended if building a Tcl 7.5 module.
Everything inside the {\tt \%\{, \%\}}
block is copied directly into the output. allowing the inclusion of
header files and additional C code." \\

Sometimes you want to really call attention to a piece of text.  You
can center it in the column like this:
\begin{center}
{\tt \_1008e614\_Vector\_p}
\end{center}
and people will really notice it.\\

\noindent
The noindent at the start of this paragraph makes it clear that it's
a continuation of the preceding text, not a new para in its own right.


Now this is an ingenious way to get a forced space.
{\tt Real~$*$} and {\tt double~$*$} are equivalent. 

Now here is another way to call attention to a line of code, but instead
of centering it, we noindent and bold it.\\

\noindent
{\bf \tt size\_t : fread ptr size nobj stream } \\

And here we have made an indented para like a definition tag (dt)
in HTML.  You don't need a surrounding list macro pair.
\begin{itemize}
\item[]  {\tt fread} reads from {\tt stream} into the array {\tt ptr} at
most {\tt nobj} objects of size {\tt size}.   {\tt fread} returns
the number of objects read. 
\end{itemize}
This concludes the definitions tag.

\subsection{How to Build Your Paper}

You have to run {\tt latex} once to prepare your references for
munging.  Then run {\tt bibtex} to build your bibliography metadata.
Then run {\tt latex} twice to ensure all references have been resolved.
If your source file is called {\tt usenixTemplate.tex} and your {\tt
  bibtex} file is called {\tt usenixTemplate.bib}, here's what you do:
{\tt \small
\begin{verbatim}
latex usenixTemplate
bibtex usenixTemplate
latex usenixTemplate
latex usenixTemplate
\end{verbatim}
}


\subsection{Last SubSection}

Well, it's getting boring isn't it.  This is the last subsection
before we wrap it up.


\section{Results}

\section{Supplementary Results}

A polite author always includes acknowledgments.  Thank everyone,
especially those who funded the work. 

\section{Discussion}

It's great when this section says that MyWonderfulApp is free software, 
available via anonymous FTP from

\begin{center}
{\tt ftp.site.dom/pub/myname/Wonderful}\\
\end{center}

Also, it's even greater when you can write that information is also 
available on the Wonderful homepage at 

\begin{center}
{\tt http://www.site.dom/\~{}myname/SWIG}
\end{center}

Now we get serious and fill in those references.  Remember you will
have to run latex twice on the document in order to resolve those
cite tags you met earlier.  This is where they get resolved.
We've preserved some real ones in addition to the template-speak.
After the bibliography you are DONE.

\section{References Cited}

\section{Author Contributions}

\end{document}






