% Copied this sample latex file from https://www.usenix.org/conferences/author-resources/paper-templates
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix,epsfig,endnotes}
\begin{document}

%don't want date printed
\date{}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf Kaggle Challenge: Predicting Rossmann Store Sales}

\author{
  {\rm Mingu Jo}\\
  University of California, Berkeley
  \and
      {\rm Wonjohn Choi}\\
      University of California, Berkeley
} % end author

\maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
\thispagestyle{empty}


\section{Abstract}
Our project attempted to apply linear regression and various machine learning techniques to a real-world problem of predicting drug store sales. Rossmann, Germany's second-largest drug store chain, has provided past sales information of 1115 Rossmann stores located across Germany. We preprocessed, feature-engineered the data, and examined 4 different statistical / machine learning analysis for forecasting sales of each store: multivariate linear regression, random forest, gradient boosting, and ____. Then, we compared the methods' predictive power by computing Root Mean Square Percentage Error (RMSPE). We found that TODO:X performed the best with a RMSPE score of TODO:XXX.


\section{Introduction}
The objective of this project is to predict 6 weeks of daily sales for 1115 Rossmann stores located across Germany using the data which was provided by Rossmann through Kaggle. The motivation of this project is the following: by reliably predicting daily sales, store managers may decrease operational costs, and create effective staff schedules (ex. by assigning more staffs during busy days). Also, we would like to identify which type of techniques be effective in a real-world sales prediction task. Before building any regression or machine learning models for the prediction, we attempted to define which features are significant to determine daily sales of stores. By performing exploratory data analysis, we assumed some features such as promotion and distance to its competitor affect the sales. % sounds little strange

(TODO: literature review) Is the literature review in the Introduction informative and organized?
To confirm our assumption and apply various techniques for the prediction, we first explored several similar efforts that made for sales prediction task in the past in order to set the basis of our study. 

- Linear Regression: The professor at Western Illionois University suggests multiple linear regression with feature selection and multicollinearity diagnostics to be a popuplar technique in sales forecasting. This model was applied for predicting the number of subscribers that a cable station can expect, which is closely related to our task. We decided to apply this technique along with partial t-test and stepwise regression to select significant features. 

- Random Forest: the random forest is a tree-based ensemble method where all trees depend on a collection of random variables. (Breiman). Breiman suggests that random forest algorithm is actually from a classification viewpoint but the approach is
applicable to regression as well. Therefore, we consider the algorithm is also applicable to our regression task. Nils Adriansson and Ingrid Mattsson in their thesis applies the algorithm into a GDP forecast task, showing its power in time-series forecasting. (Adriansson) We assume this algorithm can outperform linear regression because Breiman states that the RF does not need to assume any distribution for variables. (Breiman)

- Gradient Boosting: Random forest uses bootstrapping method for training while gradient boosting builds decision
tree over the residuals .

- KNN: (Chang, Liu, & Lai, 2008) has described a way to make
sales prediction only based on item related feature which is
more similar to our problem where only store related feature
is provided. 


the disturbances
from the mean are periodic and can effectively capture features like seasonal sales fluctuations in weather-related
equipment salesBy referencing the ____textbook____, 


As long as the techniques have already been widely researched, 

text. \\

More fascinating text. Features\endnote{Remember to use endnotes, not footnotes!} galore, plethora of promises.\\

\subsection{Dataset and Preprocessing}
Training and test dataset are given by Rossmann through Kaggle. The training set is composed of two different parts: one is \~10 million observed daily sales of different stores from 2013 to 2015, and the other is supplementary information for each store. We merged the two different parts by stores to have the training set. The training set had originally 15 features, but we extended to 19. We separated out year, month of the year, and day of the month from the feature \"date\". 

% TODO:add the feature explanatory table here.

Also, we computed average sales for each store. In addition, we noticed the feature \"CompetitionDistance\" is highly skewed to right. For the sake of linear regression, we decided to take log transformation on the feature \"CompetitionDistance\" to make sure that the relationship between explanatory variables and dependent variable is approximately linear.

% TODO:add the CompetitionDistance graph here. 

Because of several missing values in the training set, we filled them based on our logic.
- There are some stores on some date which do not have any competitors. In this case, the competitor information such as \"CompetitionDistance\" for this specific observation are missing. Therefore, we assigned some big number as 1000000 for \"CompetitionDistance\" to weaken its impact. In the similar way, we filled missing values for \"CompetitionOpenSinceYear\" and CompetitionOpenSinceMonth
- There are some stores on some date which are not in the continuing promotion. In this case, the feature \"Promo2\" is 0, and Promo2Since\[Year/Week\] are missing. We filled them with the lowest possible value to weaken their impact. 


\subsection{Exploratory Data Analysis}
To check any distinctive relationships between variables, we drew a pearson correlation coefficient plot using corrplot library. There are several interesting facts we captured. 
- \"DayOfWeek\" and \"Sales\" are negatively correlated. This implies that, as \"DayOfWeek\" approaches to Saturday and Sunday, sales would decrease because most drugstores would close on these days.
- \"LogCompetitionDistance\" and \"Sales\" are negatively correlated. In other words, lower distance to the competitor implies slightly higher sales. We assume this could occur because stores with a close distance to the competitor are mostly located in crowded regions such as a downtown city with higher sales overall.

% TODO:add the correlation plot here. 



\section{Methods}
% #####???????????
We started off by setting a benchmark to obtain a baseline for the prediction. Then, we took 3 different approaches to outperform the benchmark and former model.  

\subsection{5-fold Cross Validation} %j???????????
To generalize our predictions to limit problems of overfitting on the training set, we decided to ramdomly partition the original training observations into 5 equal sized subsamples. The training set is comprised of 4 equal sized subsamples and the rest as the validation set. For the purpose of reproducibility and comparison of the predictive powers of different models, we set the seed (with a value of 42). Our general process would be as the following:
1. Repeat cross-validation process 5 times with each of the 5 subsamples used exactly once as the validation set.
2. Compute the average of 5 different validation error rate. 
3. Predict sales on the test set using the model. 
4. Submit on Kaggle website to get the test error rate.
5. Compare the average validation error rate and the test error rate among differnent models.


\subsection{Error Metric}
As the metrics that Kaggle uses to compute test error is the Root Mean Squared Percentage Error (RMSPE), we used the same metrics to compute our validation error. It's the following:

$RMSPE = \sqrt{\frac{1}{n} \sum {(\frac{y_i - \hat y_i}{y_i})}^2}$

where ${y_i}$ is a store's sales on a single day, ${yhat_i}$ is the daily sales prediction for the store on the day, and ${n}$ is the number of (store, date) pairs in the data set. 

\subsection{Benchmark}
Our benchmark is simply the average sales of each store. Precisely, we predict daily sales of a store for a day by the average daily sales the store had in the training data set. This method does not account for many factors (whether the day was holiday, whether the store was running a promotion on the day, etc), so we expected its error rate to have a higher error rate than any other methods we applied. This method had a the validation error rate of TODO and the test error rate of TODO. 

\subsection{Linear Regression}
To apply linear Regression on the data, we decided to use R's $lm$ function. $lm$ is an implemention of multivariate linear regression. If we supply $Y \sim X$ as the input, $lm$ builds and returns a model that can be used to predict $Y$ using $X$. Under the hood, it basically solves the normal equation $\beta = (X^T X)^{-1} X^T Y$.

We first attempted to use all features available. With all features (Holiday, ..., promo2), we obtained a validation error rate of TOD and the test error rate of TODO.

\subsubsection{Feature Selection}
Before working on feature select, we took a look at the resulting lm with R function $summary(lm)$ to get some general idea of how important each variable is. DayOfWeek, Open, Promo, StateHoliday, SchoolHoliday, StoreType, Assortment, Promo2SinceWeek, Average.Sales, LogCompetitionDistance, day, month, year seemed to have high t-value with p-value less than 0.05. This intuitively made sense because each variable had some ways to affect daily sales:
\begin{enumerate}
\item StateHoliday: On a holiday, it intuitively makes sense that more people come to shop. Indeed, the coefficient is TODO.
\item TODO
\end{enumerate}

To remove useless features, we first applied Akaike information criterion (AIC) by applying R function $stepAIC$. However, this failed to remove any features, so decided to manually remove features with p value more than 0.05. This resulted in a linear model with a validation error rate of TODO and test error rate of TODO.

\subsubsection{Lasso}


\subsection{Random Forest}
Random forest is an ensemble learning method that constructs decisions tree using training set. We can use the resulting trees (random forest) to predict the daily sales for a store on a day. To apply random forest on the data, we decided to use $h2o.randomForest$ in R. 
TODO HYPER PARAMETERS



Some embedded literal typset code might 
look like the following :

{\tt \small
\begin{verbatim}
int wrap_fact(ClientData clientData,
              Tcl_Interp *interp,
              int argc, char *argv[]) {
    int result;
    int arg0;
    if (argc != 2) {
        interp->result = "wrong # args";
        return TCL_ERROR;
    }
    arg0 = atoi(argv[1]);
    result = fact(arg0);
    sprintf(interp->result,"%d",result);
    return TCL_OK;
}
\end{verbatim}
}

Now we're going to cite somebody.  Watch for the cite tag.
Here it comes~\cite{Chaum1981,Diffie1976}.  The tilde character (\~{})
in the source means a non-breaking space.  This way, your reference will
always be attached to the word that preceded it, instead of going to the
next line.

\section{Supplementary Methods}
\subsection{First SubSection}

Here's a typical figure reference.  The figure is centered at the
top of the column.  It's scaled.  It's explicitly placed.  You'll
have to tweak the numbers to get what you want.\\

% you can also use the wonderful epsfig package...
\begin{figure}[t]
\begin{center}
\begin{picture}(300,150)(0,200)
\put(-15,-30){\special{psfile = fig1.ps hscale = 50 vscale = 50}}
\end{picture}\\
\end{center}
\caption{Wonderful Flowchart}
\end{figure}

This text came after the figure, so we'll casually refer to Figure 1
as we go on our merry way.

\subsection{New Subsection}

It can get tricky typesetting Tcl and C code in LaTeX because they share
a lot of mystical feelings about certain magic characters.  You
will have to do a lot of escaping to typeset curly braces and percent
signs, for example, like this:
``The {\tt \%module} directive
sets the name of the initialization function.  This is optional, but is
recommended if building a Tcl 7.5 module.
Everything inside the {\tt \%\{, \%\}}
block is copied directly into the output. allowing the inclusion of
header files and additional C code." \\

Sometimes you want to really call attention to a piece of text.  You
can center it in the column like this:
\begin{center}
{\tt \_1008e614\_Vector\_p}
\end{center}
and people will really notice it.\\

\noindent
The noindent at the start of this paragraph makes it clear that it's
a continuation of the preceding text, not a new para in its own right.


Now this is an ingenious way to get a forced space.
{\tt Real~$*$} and {\tt double~$*$} are equivalent. 

Now here is another way to call attention to a line of code, but instead
of centering it, we noindent and bold it.\\

\noindent
{\bf \tt size\_t : fread ptr size nobj stream } \\

And here we have made an indented para like a definition tag (dt)
in HTML.  You don't need a surrounding list macro pair.
\begin{itemize}
\item[]  {\tt fread} reads from {\tt stream} into the array {\tt ptr} at
most {\tt nobj} objects of size {\tt size}.   {\tt fread} returns
the number of objects read. 
\end{itemize}
This concludes the definitions tag.

\subsection{How to Build Your Paper}

You have to run {\tt latex} once to prepare your references for
munging.  Then run {\tt bibtex} to build your bibliography metadata.
Then run {\tt latex} twice to ensure all references have been resolved.
If your source file is called {\tt usenixTemplate.tex} and your {\tt
  bibtex} file is called {\tt usenixTemplate.bib}, here's what you do:
{\tt \small
\begin{verbatim}
latex usenixTemplate
bibtex usenixTemplate
latex usenixTemplate
latex usenixTemplate
\end{verbatim}
}


\subsection{Last SubSection}

Well, it's getting boring isn't it.  This is the last subsection
before we wrap it up.


\section{Results}

\section{Supplementary Results}

A polite author always includes acknowledgments.  Thank everyone,
especially those who funded the work. 

\section{Discussion}

It's great when this section says that MyWonderfulApp is free software, 
available via anonymous FTP from

\begin{center}
{\tt ftp.site.dom/pub/myname/Wonderful}\\
\end{center}

Also, it's even greater when you can write that information is also 
available on the Wonderful homepage at 

\begin{center}
{\tt http://www.site.dom/\~{}myname/SWIG}
\end{center}

Now we get serious and fill in those references.  Remember you will
have to run latex twice on the document in order to resolve those
cite tags you met earlier.  This is where they get resolved.
We've preserved some real ones in addition to the template-speak.
After the bibliography you are DONE.

\section{References Cited}

Dehkordi-Vakil, Farideh. "Multiple Regression Analysis." DS-533. Illionis, Macomb. Lecture.

Breiman, L., (2001a). ”Random Forests”, Machine Learning 45 (1) pp. 5-32.
R.D. https://www.diva-portal.org/smash/get/diva2:785776/FULLTEXT01.pdf


\section{Author Contributions}

\end{document}






